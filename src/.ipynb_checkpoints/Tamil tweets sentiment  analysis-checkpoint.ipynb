{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08a02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "# nltk for natural language processing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41660d94",
   "metadata": {},
   "source": [
    "## Reading sentiment data in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc575ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Enna da ellam avan seyal  Mari iruku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>This movei is just like  ellam avan seyal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Padam vanthathum 13k dislike pottavaga yellam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Neraya neraya neraya... ... V era level...thala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>wow thavala sema mass....padam oru pundaikum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15739</th>\n",
       "      <td>Mixed_feelings</td>\n",
       "      <td>ivaru cinemala laam nalla tha prasuraaru...aa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15740</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Pattaya Kilaputhupaa trailer... !!!!! Get Raj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15741</th>\n",
       "      <td>Mixed_feelings</td>\n",
       "      <td>En innum trending la varala? Ennada panringa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15742</th>\n",
       "      <td>not-Tamil</td>\n",
       "      <td>Rajnikant sir plz aap india ke pm ban jaao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15743</th>\n",
       "      <td>Mixed_feelings</td>\n",
       "      <td>Enagada YouTube inum trending la add panama i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15744 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                target                                               text\n",
       "0            Negative                Enna da ellam avan seyal  Mari iruku\n",
       "1            Negative           This movei is just like  ellam avan seyal\n",
       "2            Positive    Padam vanthathum 13k dislike pottavaga yellam...\n",
       "3            Positive     Neraya neraya neraya... ... V era level...thala\n",
       "4            Positive    wow thavala sema mass....padam oru pundaikum ...\n",
       "...                ...                                                ...\n",
       "15739  Mixed_feelings    ivaru cinemala laam nalla tha prasuraaru...aa...\n",
       "15740        Positive    Pattaya Kilaputhupaa trailer... !!!!! Get Raj...\n",
       "15741  Mixed_feelings    En innum trending la varala? Ennada panringa ...\n",
       "15742       not-Tamil          Rajnikant sir plz aap india ke pm ban jaao\n",
       "15743  Mixed_feelings    Enagada YouTube inum trending la add panama i...\n",
       "\n",
       "[15744 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/Tamil_first_ready_for_sentiment.csv',sep='\\t',header=None)\n",
    "df.columns=[\"target\",\"text\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9891289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length of data is', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7feb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503b348",
   "metadata": {},
   "source": [
    "## chek for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad5f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e542aba",
   "metadata": {},
   "source": [
    "## Number of target or classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63476da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b32ef",
   "metadata": {},
   "source": [
    "## Distribution of target labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3267195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='target', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7851ee",
   "metadata": {},
   "source": [
    "# Preprocessing the text data before feeding it to ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a66cd",
   "metadata": {},
   "source": [
    "## Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "df['text']= df['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4fa447",
   "metadata": {},
   "source": [
    "## Removing any numbers from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ccf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "df['text'] = df['text'].apply(lambda x: cleaning_numbers(x))\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e00c1",
   "metadata": {},
   "source": [
    "## convert tamil text to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37009d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901c3a7",
   "metadata": {},
   "source": [
    "## convertTamiltoEnglish utility function to translate tamil tweets to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9deef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def convertTamiltoEnglish(tm):\n",
    "    #time.sleep(0.2)\n",
    "    return translator.translate(tm, src='ta', dest='en').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529bb08",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## using final result of the transaltion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.read_csv(\"data/tamiltoEnglish.csv\",sep=\",\",header=0)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87704f42",
   "metadata": {},
   "source": [
    "## Making statement text in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85305fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['english']=final_df['english'].str.lower()\n",
    "final_df['english'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53fd3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stopwrods are generally not useful for the task. Hence, they need to be removed, ex: a, the, it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb350952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stopwordlist=stopwords.words('english')\n",
    "stopwordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b5590",
   "metadata": {},
   "source": [
    "## Removing stopwrods from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "final_df['english'] = final_df['english'].apply(lambda text: cleaning_stopwords(text))\n",
    "final_df['english'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f28bbc0",
   "metadata": {},
   "source": [
    "## Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa4be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "final_df['english']= final_df['english'].apply(lambda x: cleaning_punctuations(x))\n",
    "final_df['english'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0918c",
   "metadata": {},
   "source": [
    "## Removing any numbers from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907027bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "final_df['english'] = final_df['english'].apply(lambda x: cleaning_numbers(x))\n",
    "final_df['english'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81933e",
   "metadata": {},
   "source": [
    "## tokenization of tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eafa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "final_df['english']=final_df['english'].apply(tokenizer.tokenize)\n",
    "final_df['english'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1475d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a858867",
   "metadata": {},
   "source": [
    "## Applying Stemming: Reducing a word to its stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd27e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "final_df['english']= final_df['english'].apply(lambda x: stemming_on_text(x))\n",
    "final_df['english'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa1542",
   "metadata": {},
   "source": [
    "## Lemmatization considers the context and converts the word to its meaningful base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abcd93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "final_df['english'] = final_df['english'].apply(lambda x: lemmatizer_on_text(x))\n",
    "final_df['english'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=final_df.reset_index(drop=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a8bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df,final_df],axis=1)\n",
    "df_concat=df_concat[[\"target\",\"tamil\",\"english\"]]\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16137e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos=df_concat.loc[df_concat[\"target\"]==\"Positive \"]\n",
    "df_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5729b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = df_concat['english']\n",
    "wc = WordCloud(max_words = 500 , width = 1600 , height = 800,\n",
    "              collocations=False).generate(\"\".join(data_pos.astype(str).replace(\"'\",\"\")))\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90687779",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_concat.english\n",
    "y=df_concat.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state =12312)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.astype(str)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e16b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectoriser.transform(X_train)\n",
    "#X_test  = vectoriser.transform(X_test.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.astype(str)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a05642",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test  = vectoriser.transform(X_test.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model):\n",
    "# Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb367361",
   "metadata": {},
   "source": [
    "##  target dependent variable have more than two classes i.e. multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8912edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "LRmodel = LogisticRegression(multi_class='multinomial',  max_iter = 2000,solver='lbfgs', penalty='l2', C=1.0)\n",
    "LRmodel.fit(X_train, y_train)\n",
    "model_Evaluate(LRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82bfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07481ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
